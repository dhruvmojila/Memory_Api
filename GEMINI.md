A System Designer's Playbook: Building an Open-Source LLM Memory LayerThis document outlines the complete architectural design and an AI-assisted development plan for creating a robust, open-source memory layer for Large Language Models (LLMs). The user's vision—a system that ingests memories, understands their relationships (UPDATE, EXTEND, DERIVE), and makes them available for any LLM provider—is not only feasible but represents a significant contribution to the AI development ecosystem.The selected technology stack is excellent, balancing high performance, modern development practices, and open-source-friendly components. This guide provides the architectural blueprint and a precise, step-by-step "playbook" of prompts. This playbook is designed to be fed directly to an AI coding agent, enabling a user with no prior coding experience to successfully build and deploy this complex application.Part 1: Finalized System Architecture & DesignBefore any code is generated, an unambiguous architectural plan is essential. This blueprint validates the technology stack and defines the core data models and communication protocols that will ensure the system is scalable, maintainable, and meets the strict <400ms latency requirement.1.1 Validated Architectural BlueprintThe system is composed of two primary data flows: Ingestion (writing new memories) and Retrieval (reading memories for LLM context).Data Ingestion Flow (Adding a New Memory):Client (React): A user uploads a file (e.g., meeting_notes.pdf) or submits text via a form on the React dashboard.API (FastAPI): This triggers a POST /memory request. The FastAPI backend receives the file.Orchestrator: The endpoint passes the file to a central MemoryService.Parsing: MemoryService uses a tool like PyPDFLoader to extract raw text from the document.Chunking: The text is segmented into semantically meaningful chunks using a splitter (e.g., RecursiveCharacterTextSplitter from LangChain).Embedding: An EmbeddingService (using the HuggingFace all-MiniLM-L6-v2 model) generates a 384-dimension vector embedding for each chunk.Analysis (LLM): The LLMService (using the high-speed Groq API) analyzes the chunk's content. Critically, it compares this new chunk against existing memories for that user to determine the relationship: UPDATE (supersedes old data), EXTEND (adds context), DERIVE (new insight), or NONE. This analysis is best controlled using a structured prompting library like DSPY.Storage (Graph): The GraphService (interfacing with Neo4j) executes a database transaction to write the new Memory node, link it to the User, and create the appropriate relationship (e.g., ``) to other nodes.Real-time (WebSocket): Upon successful commit, the GraphService notifies a central WebSocketManager.Broadcast: The WebSocketManager finds the specific user_id "room" and broadcasts a GRAPH_UPDATE message containing the new graph structure to the user's dashboard.1Data Retrieval Flow (Hybrid Search for LLM Context):Client (LLM API Call): An external service or the user's chat interface makes a GET /search call to the API with a natural language query (e.g., "What's the status of Project Alpha?").Embedding: The EmbeddingService generates a vector for the query string.Phase 1: Semantic Search (Vector): The GraphService executes a Cypher query against the Neo4j vector index. It finds the top 5 Memory nodes that are both is_latest: true and semantically closest to the query vector.Phase 2: Graph Traversal (Graph): The system takes these 5 "seed nodes" and performs a 1- or 2-hop graph traversal. This "GraphRAG" step pulls in connected memories—such as context added via or insights from—that a simple vector search would have missed.Context Assembly: The retrieved nodes and their relationships (the subgraph) are serialized into a concise text block.Response: The API returns this text block. This context is then prepended to the user's actual prompt to their chosen LLM (e.g., Groq, OpenAI), giving the LLM the necessary memory to provide an accurate answer.1.2 The Graph Schema: The Heart of the MemoryThe single most critical design choice is the graph schema. A well-designed schema makes <400ms queries possible; a poor one makes them impossible. This system will use a Labeled Property Graph (LPG) model, implemented in Neo4j.Node Labels:User: The owner of the memory.Properties: user_id (indexed), usernameDocument: The original source file.Properties: doc_id (indexed), filename, source_type ('pdf', 'text'), created_atMemory: A single chunk of text.Properties: memory_id (indexed), text_content, embedding (vector), created_at, is_latest (boolean, indexed)Entity: A named entity (person, place, project).Properties: entity_id (indexed), name, type ('PERSON', 'PROJECT')Relationship Types:(User)-->(Memory)(Memory)-->(Document)(Memory)-->(Entity)(Memory {id: 'B'})-->(Memory {id: 'A'})(Memory {id: 'B'})-->(Memory {id: 'A'})(Memory {id: 'C'})-->(Memory {id: 'A'})(Memory {id: 'C'})-->(Memory {id: 'B'})Critical Design Pattern: The is_latest FlagThe is_latest boolean property on the Memory node is the key to achieving the <400ms latency target. When an UPDATE relationship is created (e.g., (B)-->(A)), the system also executes a command to SET A.is_latest = false.This is a form of de-normalization. Instead of forcing the retrieval query to perform a complex graph-traversal at read-time (e.g., "find all memories AND check that no other memory UPDATES them"), the system pre-computes this state during the write. This makes the read query significantly faster and simpler, as it can now lead with a highly-indexed filter: MATCH (m:Memory WHERE m.is_latest = true)... This dramatically prunes the search space before the expensive vector search is performed.Table 1: Neo4j Graph Schema and Indexing StrategyNode LabelPropertyIndex TypeCypher Command (for AI Agent)Useruser_idRANGECREATE INDEX user_id_index IF NOT EXISTS FOR (u:User) ON (u.user_id);Memorymemory_idRANGECREATE INDEX memory_id_index IF NOT EXISTS FOR (m:Memory) ON (m.memory_id);Memoryis_latestRANGECREATE INDEX memory_latest_index IF NOT EXISTS FOR (m:Memory) ON (m.is_latest);EntitynameRANGECREATE INDEX entity_name_index IF NOT EXISTS FOR (e:Entity) ON (e.name);MemoryembeddingVECTORCREATE VECTOR INDEX memory_embedding_index IF NOT EXISTS FOR (m:Memory) ON (m.embedding) OPTIONS {indexConfig: { 'vector.dimensions': 384, 'vector.similarity_function': 'cosine' }};1.3 The Real-Time Data Flow: The "Graph Update Bus"To power the real-time dashboard, a one-way "Graph Update Bus" will be implemented using WebSockets. This is not a generic chat system but a dedicated notification channel.Standard WebSocket connection managers often use a simple List.2 This is inefficient and a major security flaw for this use case, as it would require broadcasting all graph updates to all connected users.The correct design, derived from server-side best practices, is to use a Dict].1 This dictionary maps a user_id (the "room") to a list of that user's active WebSocket connections. When a memory is updated for user_id_123, the WebSocketManager looks up user_id_123 in the dictionary and broadcasts the update only to the connections in that list. This ensures data is perfectly isolated between users.Table 2: WebSocket Event ContractEvent Typedata Payload (JSON Structure)DescriptionGRAPH_UPDATE{ "nodes": [...], "edges": [...] }Fired when new nodes/relationships are added. Contains the entire updated graph for the user to render.NODE_FOCUS{ "node_id": "..." }Fired on GET /search to instruct the frontend to pan/zoom to the most relevant node.ERROR{ "code": "...", "message": "..." }Reports an error in the WebSocket connection or authentication.1.4 API Endpoint SpecificationThis table summarizes the complete API surface for the application, providing a clear reference for both backend and frontend development.Table 3: API Endpoint SpecificationMethodPathAuthRequest Body / Query ParamsDescriptionPOST/memoryJWT (Header)File: (pdf, txt, docx) or text: strIngests a new memory, chunks it, and updates the graph.GET/searchJWT (Header)q: str (query parameter)Performs hybrid vector+graph search and returns a subgraph for LLM context.GET/graphJWT (Header)user_id: strRetrieves the user's entire graph for initial dashboard load.WS/wsJWT (Query Param)token: str (query parameter)Establishes the real-time graph update channel.
